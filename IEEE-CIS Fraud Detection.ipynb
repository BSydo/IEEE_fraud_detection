{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data was loaded\n",
      "Mem. usage decreased to 668.22 Mb (66.2% reduction)\n",
      "without GPU: 51.23994868199998\n",
      "Mem. usage decreased to 668.22 Mb (0.0% reduction)\n",
      "without GPU: 8.530972062000274\n",
      "Mem. usage decreased to 583.43 Mb (65.6% reduction)\n",
      "without GPU: 38.61772417599968\n",
      "Mem. usage decreased to 583.43 Mb (0.0% reduction)\n",
      "without GPU: 6.836222655999791\n",
      "training set shape: (590540, 433)\n",
      "test set shape: (506691, 432)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import jit, cuda \n",
    "from timeit import default_timer as timer \n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "input_train_tr = pd.read_csv(f'{sys.path[0]}/Input/ieee-fraud-detection/train_transaction.csv'\n",
    "                             , index_col='TransactionID') #train_transaction\n",
    "input_test_tr = pd.read_csv(f'{sys.path[0]}/Input/ieee-fraud-detection/test_transaction.csv'\n",
    "                            , index_col='TransactionID')#test_transaction\n",
    "input_train_id = pd.read_csv(f'{sys.path[0]}/Input/ieee-fraud-detection/train_identity.csv'\n",
    "                             , index_col='TransactionID')#train_identity\n",
    "input_test_id = pd.read_csv(f'{sys.path[0]}/Input/ieee-fraud-detection/test_identity.csv'\n",
    "                            , index_col='TransactionID')#test_identity\n",
    "sample_submission = pd.read_csv(f'{sys.path[0]}/output/sample_submission.csv'\n",
    "                                , index_col='TransactionID')\n",
    "print('data was loaded')\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start = timer()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\\\n",
    "                      .format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    print(\"without GPU:\", timer()-start) \n",
    "    return df\n",
    "\n",
    "@jit(target =\"cuda\")\n",
    "def reduce_mem_usage2(df, verbose=True):\n",
    "    start = timer()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\\\n",
    "                      .format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    print(\"with GPU:\", timer()-start) \n",
    "    return df\n",
    "\n",
    "train = input_train_tr.merge(input_train_id, how='left', left_index=True, right_index=True)\n",
    "del input_train_tr, input_train_id\n",
    "gc.collect()\n",
    "test = input_test_tr.merge(input_test_id, how='left', left_index=True, right_index=True)\n",
    "del input_test_tr, input_test_id\n",
    "gc.collect\n",
    "\n",
    "train1 = reduce_mem_usage(train)\n",
    "train = reduce_mem_usage(train)\n",
    "\n",
    "test1 = reduce_mem_usage(test)\n",
    "test = reduce_mem_usage(test)\n",
    "\n",
    "print('training set shape:', train.shape)\n",
    "print('test set shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 features are going to be dropped for being useless\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function gc.collect(generation=2)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_drop=['V300','V309','V111','C3','V124','V106','V125','V315','V134','V102','V123','V316','V113',\n",
    "              'V136','V305','V110','V299','V289','V286','V318','V103','V304','V116','V29','V284','V293',\n",
    "              'V137','V295','V301','V104','V311','V115','V109','V119','V321','V114','V133','V122','V319',\n",
    "              'V105','V112','V118','V117','V121','V108','V135','V320','V303','V297','V120']\n",
    "\n",
    "\n",
    "print('{} features are going to be dropped for being useless'.format(len(cols_to_drop)))\n",
    "\n",
    "train = train.drop(cols_to_drop, axis=1)\n",
    "test = test.drop(cols_to_drop, axis=1)\n",
    "del cols_to_drop\n",
    "gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['P_isproton']=(train['P_emaildomain']=='protonmail.com')\n",
    "train['R_isproton']=(train['R_emaildomain']=='protonmail.com')\n",
    "test['P_isproton']=(test['P_emaildomain']=='protonmail.com')\n",
    "test['R_isproton']=(test['R_emaildomain']=='protonmail.com')\n",
    "\n",
    "train['nulls1'] = train.isna().sum(axis=1)\n",
    "test['nulls1'] = test.isna().sum(axis=1)\n",
    "\n",
    "a = np.zeros(train.shape[0])\n",
    "train[\"lastest_browser\"] = a\n",
    "a = np.zeros(test.shape[0])\n",
    "test[\"lastest_browser\"] = a\n",
    "def setbrowser(df):\n",
    "    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n",
    "    return df\n",
    "train=setbrowser(train)\n",
    "test=setbrowser(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['card1_count_full'] = train['card1'].map(pd.concat([train['card1'], test['card1']]\n",
    "                                                         , ignore_index=True).value_counts(dropna=False))\n",
    "test['card1_count_full'] = test['card1'].map(pd.concat([train['card1'], test['card1']]\n",
    "                                                       , ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "train['card2_count_full'] = train['card2'].map(pd.concat([train['card2'], test['card2']]\n",
    "                                                         , ignore_index=True).value_counts(dropna=False))\n",
    "test['card2_count_full'] = test['card2'].map(pd.concat([train['card2'], test['card2']]\n",
    "                                                       , ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "train['card3_count_full'] = train['card3'].map(pd.concat([train['card3'], test['card3']]\n",
    "                                                         , ignore_index=True).value_counts(dropna=False))\n",
    "test['card3_count_full'] = test['card3'].map(pd.concat([train['card3'], test['card3']]\n",
    "                                                       , ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "train['card4_count_full'] = train['card4'].map(pd.concat([train['card4'], test['card4']]\n",
    "                                                         , ignore_index=True).value_counts(dropna=False))\n",
    "test['card4_count_full'] = test['card4'].map(pd.concat([train['card4'], test['card4']]\n",
    "                                                       , ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "train['card5_count_full'] = train['card5'].map(pd.concat([train['card5'], test['card5']]\n",
    "                                                         , ignore_index=True).value_counts(dropna=False))\n",
    "test['card5_count_full'] = test['card5'].map(pd.concat([train['card5'], test['card5']]\n",
    "                                                       , ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "train['card6_count_full'] = train['card6'].map(pd.concat([train['card6'], test['card6']]\n",
    "                                                         , ignore_index=True).value_counts(dropna=False))\n",
    "test['card6_count_full'] = test['card6'].map(pd.concat([train['card6'], test['card6']]\n",
    "                                                       , ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "train['addr1_count_full'] = train['addr1'].map(pd.concat([train['addr1'], test['addr1']]\n",
    "                                                         , ignore_index=True).value_counts(dropna=False))\n",
    "test['addr1_count_full'] = test['addr1'].map(pd.concat([train['addr1'], test['addr1']]\n",
    "                                                       , ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "train['addr2_count_full'] = train['addr2'].map(pd.concat([train['addr2'], test['addr2']]\n",
    "                                                         , ignore_index=True).value_counts(dropna=False))\n",
    "test['addr2_count_full'] = test['addr2'].map(pd.concat([train['addr2'], test['addr2']]\n",
    "                                                       , ignore_index=True).value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "train['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "test['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "test['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "train['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\n",
    "train['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\n",
    "train['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\n",
    "train['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n",
    "\n",
    "test['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\n",
    "test['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\n",
    "test['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\n",
    "test['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n",
    "\n",
    "train['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\n",
    "train['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\n",
    "train['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\n",
    "train['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "test['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\n",
    "test['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\n",
    "test['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\n",
    "test['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "train['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\n",
    "train['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\n",
    "train['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\n",
    "train['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "test['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\n",
    "test['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\n",
    "test['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\n",
    "test['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Transaction_day_of_week'] = np.floor((train['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "test['Transaction_day_of_week'] = np.floor((test['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "\n",
    "train['Transaction_hour_of_day'] = np.floor(train['TransactionDT'] / 3600) % 24\n",
    "test['Transaction_hour_of_day'] = np.floor(test['TransactionDT'] / 3600) % 24\n",
    "\n",
    "train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "test['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function gc.collect(generation=2)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "for feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2'\n",
    "                ,'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n",
    "\n",
    "    f1, f2 = feature.split('__')\n",
    "    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n",
    "    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n",
    "\n",
    "    le =preprocessing.LabelEncoder()\n",
    "    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n",
    "    train[feature] = le.transform(list(train[feature].astype(str).values))\n",
    "    test[feature] = le.transform(list(test[feature].astype(str).values))\n",
    "    \n",
    "for feature in ['id_01', 'id_31', 'id_33', 'id_35']:\n",
    "    # Count encoded separately for train and test\n",
    "    train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n",
    "    test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))\n",
    "    \n",
    "category_features=[\"ProductCD\",\"P_emaildomain\",\n",
    "                   \"R_emaildomain\",\"M1\",\"M2\",\"M3\",\"M4\",\"M5\",\"M6\",\"M7\",\"M8\",\"M9\",\"DeviceType\",\"DeviceInfo\",\"id_12\",\n",
    "                   \"id_13\",\"id_14\",\"id_15\",\"id_16\",\"id_17\",\"id_18\",\"id_19\",\"id_20\",\"id_21\",\"id_22\",\"id_23\",\"id_24\"\n",
    "                   ,\"id_25\",\"id_26\",\"id_27\",\"id_28\",\"id_29\",\"id_30\",\"id_32\",\"id_34\", 'id_36'\n",
    "                   \"id_37\",\"id_38\"]\n",
    "for c in category_features:\n",
    "    train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]]\n",
    "                                                                  , ignore_index=True).value_counts(dropna=False))\n",
    "    test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]]\n",
    "                                                                , ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "del le\n",
    "gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelling done.\n"
     ]
    }
   ],
   "source": [
    "y_train = train['isFraud'].copy()\n",
    "X_train = train.drop('isFraud', axis=1)\n",
    "X_test = test.copy()\n",
    "del train, test\n",
    "\n",
    "#fill in mean for floats\n",
    "for c in X_train.columns:\n",
    "    if X_train[c].dtype=='float16' or  X_train[c].dtype=='float32' or  X_train[c].dtype=='float64':\n",
    "        X_train[c].fillna(X_train[c].mean())\n",
    "        X_test[c].fillna(X_train[c].mean())\n",
    "\n",
    "#fill in -999 for categoricals\n",
    "X_train = X_train.fillna(-999)\n",
    "X_test = X_test.fillna(-999)\n",
    "# Label Encoding\n",
    "for f in X_train.columns:\n",
    "    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n",
    "        X_train[f] = lbl.transform(list(X_train[f].values))\n",
    "        X_test[f] = lbl.transform(list(X_test[f].values))  \n",
    "        \n",
    "print('Labelling done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=4, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit,KFold\n",
    "n_fold = 4\n",
    "folds = KFold(n_splits=n_fold,shuffle=True)\n",
    "\n",
    "print(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "finish train\n",
      "finish pred\n",
      "ROC accuracy: 0.9708960995224276\n",
      "1\n",
      "finish train\n",
      "finish pred\n",
      "ROC accuracy: 0.969022961029457\n",
      "2\n",
      "finish train\n",
      "finish pred\n",
      "ROC accuracy: 0.9714071313404432\n",
      "3\n",
      "finish train\n",
      "finish pred\n",
      "ROC accuracy: 0.9682233633398203\n"
     ]
    }
   ],
   "source": [
    "lgb_submission=sample_submission.copy()\n",
    "lgb_submission['isFraud'] = 0\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train)):\n",
    "    print(fold_n)\n",
    "    \n",
    "    X_train_, X_valid = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "    y_train_, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "    \n",
    "    lgbclf = lgb.LGBMClassifier(\n",
    "        num_leaves= 512,\n",
    "        n_estimators=512,\n",
    "        max_depth=9,\n",
    "        learning_rate=0.064,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        boosting_type= \"gbdt\",\n",
    "        reg_alpha=0.3,\n",
    "        reg_lamdba=0.243\n",
    "    )\n",
    "    \n",
    "    X_train_, X_valid = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "    y_train_, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "    lgbclf.fit(X_train_,y_train_)\n",
    "    \n",
    "    del X_train_,y_train_\n",
    "    print('finish train')\n",
    "    pred=lgbclf.predict_proba(X_test)[:,1]\n",
    "    val=lgbclf.predict_proba(X_valid)[:,1]\n",
    "    print('finish pred')\n",
    "    del lgbclf, X_valid\n",
    "    print('ROC accuracy: {}'.format(roc_auc_score(y_valid, val)))\n",
    "    del val,y_valid\n",
    "    lgb_submission['isFraud'] = lgb_submission['isFraud']+pred/n_fold\n",
    "    del pred\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "ROC accuracy: 0.9688593278582648\n",
      "1\n",
      "ROC accuracy: 0.9702893074255019\n",
      "2\n",
      "ROC accuracy: 0.9701172778278406\n",
      "3\n",
      "ROC accuracy: 0.9703050694213604\n"
     ]
    }
   ],
   "source": [
    "xgb_submission=sample_submission.copy()\n",
    "xgb_submission['isFraud'] = 0\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train)):\n",
    "    print(fold_n)\n",
    "    xgbclf = xgb.XGBClassifier(\n",
    "        n_estimators=512,\n",
    "        max_depth=16,\n",
    "        learning_rate=0.014,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        missing=-999,\n",
    "        tree_method='gpu_hist',\n",
    "        reg_alpha=0.3,\n",
    "        reg_lamdba=0.243\n",
    "    )\n",
    "    \n",
    "    X_train_, X_valid = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "    y_train_, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "    xgbclf.fit(X_train_,y_train_)\n",
    "    del X_train_,y_train_\n",
    "    pred=xgbclf.predict_proba(X_test)[:,1]\n",
    "    val=xgbclf.predict_proba(X_valid)[:,1]\n",
    "    del xgbclf, X_valid\n",
    "    print('ROC accuracy: {}'.format(roc_auc_score(y_valid, val)))\n",
    "    del val,y_valid\n",
    "    xgb_submission['isFraud'] = xgb_submission['isFraud']+pred/n_fold\n",
    "    del pred\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "lgbclf = lgb.LGBMRegressor(\n",
    "        num_leaves= 512,\n",
    "        n_estimators=512,\n",
    "        max_depth=9,\n",
    "        learning_rate=0.064,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        boosting_type= \"gbdt\",\n",
    "        reg_alpha=0.3,\n",
    "        reg_lamdba=0.243,\n",
    "        metric=\"AUC\"\n",
    "    )\n",
    "xgbclf = xgb.XGBRegressor(\n",
    "        n_estimators=512,\n",
    "        max_depth=16,\n",
    "        learning_rate=0.014,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        missing=-999,\n",
    "        tree_method='gpu_hist',\n",
    "        reg_alpha=0.3,\n",
    "        reg_lamdba=0.243\n",
    "    )\n",
    "rfclf = RandomForestRegressor(n_estimators=512,\n",
    "                              max_depth=5, \n",
    "                                max_features='sqrt', \n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [[lgbclf,xgbclf], # Level 1\n",
    "          [rfclf]] # Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.replace(np.inf,-999)\n",
    "X_train=X_train.replace(-np.inf,-999)\n",
    "X_test=X_test.replace(np.inf,-999)\n",
    "X_test=X_test.replace(-np.inf,-999)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "feature_cols=X_train.columns.values.tolist()\n",
    "\n",
    "\n",
    "feature_imp=sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), feature_cols), \n",
    "             reverse=True)\n",
    "del rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce dimention finished\n"
     ]
    }
   ],
   "source": [
    "feature_imp=[x[1] for x in feature_imp]\n",
    "X_train=X_train[feature_imp[:250]].values\n",
    "X_test=X_test[feature_imp[:250]].values\n",
    "\n",
    "print('reduce dimention finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Start of Level 0 ======================\n",
      "Input Dimensionality 250 at Level 0 \n",
      "2 models included in Level 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/4 , model 0 , auc===0.946912 \n",
      "Fold 1/4 , model 1 , auc===0.959897 \n",
      "=========== end of fold 1 in level 0 ===========\n",
      "Fold 2/4 , model 0 , auc===0.946255 \n",
      "Fold 2/4 , model 1 , auc===0.960575 \n",
      "=========== end of fold 2 in level 0 ===========\n",
      "Fold 3/4 , model 0 , auc===0.952921 \n",
      "Fold 3/4 , model 1 , auc===0.965146 \n",
      "=========== end of fold 3 in level 0 ===========\n",
      "Fold 4/4 , model 0 , auc===0.948757 \n",
      "Fold 4/4 , model 1 , auc===0.962606 \n",
      "=========== end of fold 4 in level 0 ===========\n",
      "Output dimensionality of level 0 is 2 \n",
      "====================== End of Level 0 ======================\n",
      " level 0 lasted 3103.939995 seconds \n",
      "====================== Start of Level 1 ======================\n",
      "Input Dimensionality 2 at Level 1 \n",
      "1 models included in Level 1 \n",
      "Fold 1/4 , model 0 , auc===0.957605 \n",
      "=========== end of fold 1 in level 1 ===========\n",
      "Fold 2/4 , model 0 , auc===0.957131 \n",
      "=========== end of fold 2 in level 1 ===========\n",
      "Fold 3/4 , model 0 , auc===0.963316 \n",
      "=========== end of fold 3 in level 1 ===========\n",
      "Fold 4/4 , model 0 , auc===0.959326 \n",
      "=========== end of fold 4 in level 1 ===========\n",
      "Output dimensionality of level 1 is 1 \n",
      "====================== End of Level 1 ======================\n",
      " level 1 lasted 803.336027 seconds \n",
      "====================== End of fit ======================\n",
      " fit() lasted 3907.294236 seconds \n"
     ]
    }
   ],
   "source": [
    "from pystacknet.pystacknet import StackNetClassifier\n",
    "\n",
    "# Specify parameters for stacked model and begin training\n",
    "model = StackNetClassifier(models, \n",
    "                           metric=\"auc\", \n",
    "                           folds=4,\n",
    "                           restacking=False,\n",
    "                           use_retraining=True,\n",
    "                           use_proba=True, # To use predict_proba after training\n",
    "                           random_state=0,\n",
    "                           n_jobs=-1, \n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the entire model tree\n",
    "del models\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Start of Level 0 ======================\n",
      "1 estimators included in Level 0 \n",
      "====================== Start of Level 1 ======================\n",
      "1 estimators included in Level 1 \n"
     ]
    }
   ],
   "source": [
    "# Write predictions to csv\n",
    "stack_submission = sample_submission.copy()\n",
    "preds = model.predict_proba(X_test)[:, 1]\n",
    "stack_submission['isFraud'] = preds\n",
    "#sub.to_csv(f\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble=sample_submission.copy()\n",
    "#ensemble.isFraud=lgb_submission*0.45+xgb_submission*0.45+stack_submission*0.1\n",
    "#ensemble.isFraud=lgb_submission*0.5+xgb_submission*0.5\n",
    "#ensemble.isFraud=xgb_submission\n",
    "ensemble.isFraud=lgb_submission\n",
    "ensemble.to_csv('xgb_lgb_stacking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
