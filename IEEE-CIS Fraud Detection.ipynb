{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE-CIS Fraud Detection - data analisys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Detailed infomation about competition:\n",
    "https://www.kaggle.com/c/ieee-fraud-detection/overview\n",
    "\n",
    "    Data analitycs for the competition dataset\n",
    "    Done before modelling for features investigation and finding logical correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libs, ignoring all warnings, defining plots style and color palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import wget\n",
    "import zipfile\n",
    "import string\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes\\\n",
    "                    , metrics, svm, decomposition, ensemble\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "#from keras import layers, models, optimizers\n",
    "import xgboost \n",
    "\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "__location__ = sys.path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Data import\n",
    "\n",
    "Train datasets:\n",
    "- train_identity.csv -- shape(144233, 41) -- additional dataset, connected with transactions by TransactionID\n",
    "- train_transaction.csv -- shape(590540, 394) -- main dataset with transactional history\n",
    "\n",
    "Test datasets:\n",
    "- test_identity.csv -- shape(141907, 41)\n",
    "- test_transaction.csv -- shape(506691, 393)\n",
    "\n",
    "Finals:\n",
    "- sample_submission.csv -- submission finals file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_identity = pd.read_csv(f'{__location__}/Input/ieee-fraud-detection/train_identity.csv')\n",
    "train_transaction = pd.read_csv(f'{__location__}/Input/ieee-fraud-detection/train_transaction.csv')\n",
    "test_identity = pd.read_csv(f'{__location__}/Input/ieee-fraud-detection/test_identity.csv')\n",
    "test_transaction = pd.read_csv(f'{__location__}/Input/ieee-fraud-detection/test_transaction.csv')\n",
    "ss = pd.read_csv(f'{__location__}/Input/ieee-fraud-detection/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_num_cols = [train_transaction.columns[n] for n,i in enumerate(train_transaction.dtypes)\n",
    "                if i not in ('int64','float64')]\n",
    "#train_transaction[non_num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "for i in non_num_cols:\n",
    "    train_transaction['mod_'+i] = encoder.fit_transform(train_transaction[i].fillna('0'))\n",
    "    test_transaction['mod_'+i] = encoder.fit_transform(test_transaction[i].fillna('0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        TransactionID  TransactionDT  TransactionAmt  card1  card2  card3  \\\n",
      "0                 NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "1                 NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "2                 NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "3                 NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "4                 NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "5                 NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "6                 NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "7                 NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "8                 NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "9                 NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "10                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "11                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "12                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "13                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "14                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "15                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "16                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "17                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "18                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "19                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "20                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "21                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "22                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "23                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "24                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "25                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "26                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "27                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "28                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "29                NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "...               ...            ...             ...    ...    ...    ...   \n",
      "590510            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590511            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590512            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590513            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590514            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590515            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590516            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590517            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590518            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590519            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590520            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590521            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590522            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590523            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590524            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590525            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590526            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590527            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590528            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590529            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590530            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590531            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590532            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590533            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590534            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590535            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590536            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590537            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590538            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "590539            NaN            NaN             NaN    NaN    NaN    NaN   \n",
      "\n",
      "        card5  addr1  addr2  dist1  ...  mod_R_emaildomain  mod_M1  mod_M2  \\\n",
      "0         NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "1         NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "2         NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "3         NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "4         NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "5         NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "6         NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "7         NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "8         NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "9         NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "10        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "11        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "12        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "13        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "14        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "15        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "16        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "17        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "18        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "19        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "20        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "21        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "22        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "23        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "24        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "25        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "26        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "27        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "28        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "29        NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "...       ...    ...    ...    ...  ...                ...     ...     ...   \n",
      "590510    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590511    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590512    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590513    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590514    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590515    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590516    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590517    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590518    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590519    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590520    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590521    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590522    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590523    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590524    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590525    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590526    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590527    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590528    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590529    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590530    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590531    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590532    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590533    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590534    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590535    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590536    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590537    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590538    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "590539    NaN    NaN    NaN    NaN  ...                NaN     NaN     NaN   \n",
      "\n",
      "        mod_M3  mod_M4  mod_M5  mod_M6  mod_M7  mod_M8  mod_M9  \n",
      "0          NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "1          NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "2          NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "3          NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "4          NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "5          NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "6          NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "7          NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "8          NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "9          NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "10         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "11         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "12         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "13         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "14         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "15         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "16         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "17         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "18         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "19         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "20         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "21         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "22         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "23         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "24         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "25         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "26         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "27         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "28         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "29         NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "...        ...     ...     ...     ...     ...     ...     ...  \n",
      "590510     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590511     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590512     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590513     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590514     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590515     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590516     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590517     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590518     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590519     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590520     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590521     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590522     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590523     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590524     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590525     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590526     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590527     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590528     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590529     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590530     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590531     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590532     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590533     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590534     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590535     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590536     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590537     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590538     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "590539     NaN     NaN     NaN     NaN     NaN     NaN     NaN  \n",
      "\n",
      "[590540 rows x 393 columns]\n"
     ]
    }
   ],
   "source": [
    "col_list = [x for x in train_transaction.columns if x not in non_num_cols+['isFraud']]\n",
    "\n",
    "num = train_transaction[col_list]._get_numeric_data()\n",
    "print(num[num < 0])# = 0 \n",
    "\n",
    "#x_train, y_train, x_valid, y_valid = model_selection.train_test_split(train_transaction[col_list].fillna(0)\n",
    "#                                                                      , train_transaction['isFraud'])\n",
    "\n",
    "#print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-268e19d300e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Naive Bayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive_bayes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"NB, Count Vectors: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Linear Classifier on Count Vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-80ae62c3045c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vector_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_neural_net\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# fit the training dataset on the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# predict the labels on validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    611\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    612\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), x_train, x_valid, y_train)\n",
    "print (\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), x_train, x_valid, y_train)\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), x_train, x_valid, y_train)\n",
    "print (\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), x_train, x_valid, y_train)\n",
    "print (\"Xgb, Count Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier \n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy = train_model(classifier, xtrain_tfidf_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], \n",
    "                                       trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print (\"CNN, Word Embeddings\",  accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
